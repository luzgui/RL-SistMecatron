{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e2a147b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/acr/Documents/Projeto Sistemas Mecatrónicos/RL-SistMecatron'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec4ef092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/acr/Documents/Projeto Sistemas Mecatrónicos/RL-SistMecatron\n"
     ]
    }
   ],
   "source": [
    "%cd /home/acr/Documents/Projeto Sistemas Mecatrónicos/RL-SistMecatron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e052221",
   "metadata": {},
   "source": [
    "# Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0086cbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==2.3.0 (from versions: 2.5.0rc0, 2.5.0rc1, 2.5.0rc2, 2.5.0rc3, 2.5.0, 2.5.1, 2.5.2, 2.6.0rc0, 2.6.0rc1, 2.6.0rc2, 2.6.0, 2.6.1, 2.6.2, 2.7.0rc0, 2.7.0rc1, 2.7.0)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for tensorflow==2.3.0\u001b[0m\n",
      "Requirement already satisfied: gym in /home/acr/anaconda3/lib/python3.9/site-packages (0.21.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /home/acr/anaconda3/lib/python3.9/site-packages (from gym) (1.20.3)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/acr/anaconda3/lib/python3.9/site-packages (from gym) (2.0.0)\n",
      "Requirement already satisfied: keras in /home/acr/anaconda3/lib/python3.9/site-packages (2.7.0)\n",
      "Requirement already satisfied: keras-rl2 in /home/acr/anaconda3/lib/python3.9/site-packages (1.0.5)\n",
      "Requirement already satisfied: tensorflow in /home/acr/anaconda3/lib/python3.9/site-packages (from keras-rl2) (2.7.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/acr/anaconda3/lib/python3.9/site-packages (from tensorflow->keras-rl2) (3.3.0)\n",
      "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /home/acr/anaconda3/lib/python3.9/site-packages (from tensorflow->keras-rl2) (0.4.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /home/acr/anaconda3/lib/python3.9/site-packages (from tensorflow->keras-rl2) (3.19.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /home/acr/anaconda3/lib/python3.9/site-packages (from tensorflow->keras-rl2) (2.7.0)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in /home/acr/anaconda3/lib/python3.9/site-packages (from tensorflow->keras-rl2) (1.0.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/acr/anaconda3/lib/python3.9/site-packages (from tensorflow->keras-rl2) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /home/acr/anaconda3/lib/python3.9/site-packages (from tensorflow->keras-rl2) (1.20.3)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/acr/anaconda3/lib/python3.9/site-packages (from tensorflow->keras-rl2) (1.12.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/acr/anaconda3/lib/python3.9/site-packages (from tensorflow->keras-rl2) (1.1.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/acr/anaconda3/lib/python3.9/site-packages (from tensorflow->keras-rl2) (0.2.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /home/acr/anaconda3/lib/python3.9/site-packages (from tensorflow->keras-rl2) (0.22.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /home/acr/anaconda3/lib/python3.9/site-packages (from tensorflow->keras-rl2) (1.1.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/acr/anaconda3/lib/python3.9/site-packages (from tensorflow->keras-rl2) (1.42.0)\n",
      "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /home/acr/anaconda3/lib/python3.9/site-packages (from tensorflow->keras-rl2) (2.7.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/acr/anaconda3/lib/python3.9/site-packages (from tensorflow->keras-rl2) (1.6.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.32.0 in /home/acr/anaconda3/lib/python3.9/site-packages (from tensorflow->keras-rl2) (0.37.0)\n",
      "Requirement already satisfied: libclang>=9.0.1 in /home/acr/anaconda3/lib/python3.9/site-packages (from tensorflow->keras-rl2) (12.0.0)\n",
      "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /home/acr/anaconda3/lib/python3.9/site-packages (from tensorflow->keras-rl2) (2.0)\n",
      "Requirement already satisfied: tensorboard~=2.6 in /home/acr/anaconda3/lib/python3.9/site-packages (from tensorflow->keras-rl2) (2.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/acr/anaconda3/lib/python3.9/site-packages (from tensorflow->keras-rl2) (3.10.0.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/acr/anaconda3/lib/python3.9/site-packages (from tensorflow->keras-rl2) (1.16.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/acr/anaconda3/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow->keras-rl2) (2.3.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/acr/anaconda3/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow->keras-rl2) (3.3.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/acr/anaconda3/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow->keras-rl2) (2.26.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/acr/anaconda3/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow->keras-rl2) (58.0.4)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/acr/anaconda3/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow->keras-rl2) (1.8.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/acr/anaconda3/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow->keras-rl2) (0.4.6)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/acr/anaconda3/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow->keras-rl2) (2.0.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/acr/anaconda3/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow->keras-rl2) (0.6.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/acr/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow->keras-rl2) (4.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/acr/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow->keras-rl2) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/acr/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow->keras-rl2) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/acr/anaconda3/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow->keras-rl2) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/acr/anaconda3/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow->keras-rl2) (4.8.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/acr/anaconda3/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow->keras-rl2) (3.6.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/acr/anaconda3/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow->keras-rl2) (0.4.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/acr/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow->keras-rl2) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/acr/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow->keras-rl2) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/acr/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow->keras-rl2) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/acr/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow->keras-rl2) (3.2)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/acr/anaconda3/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow->keras-rl2) (3.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.3.0\n",
    "!pip install gym\n",
    "!pip install keras\n",
    "!pip install keras-rl2\n",
    "# !pip install gym[atari]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66992cdd",
   "metadata": {},
   "source": [
    "# Creating a random environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cada7c7",
   "metadata": {},
   "source": [
    "#### The goal is to balance the pole up, moving the cart to the left or right . For each step it takes a point, for a maximum of 200 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "647f29d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c211a5b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version +978d2ce)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('SpaceInvaders-v0') # To be able to get all the environments I had to clone the openAIGym repository and pip install -e.[all]\n",
    "height, width, channels = env.observation_space.shape\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c390347",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.unwrapped.get_action_meanings() # There are 6 possible actions that the spaceships can do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cceb1c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# help(env.compute_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66b5b20",
   "metadata": {},
   "source": [
    "#### Now we're just setting some random episodes where there is no learning from each of the episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0fa8a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/acr/anaconda3/lib/python3.9/site-packages/gym/envs/atari/environment.py:255: UserWarning: \u001b[33mWARN: We strongly suggest supplying `render_mode` when constructing your environment, e.g., gym.make(ID, render_mode='human'). Using `render_mode` provides access to proper scaling, audio support, and proper framerates.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score: 135.0\n",
      "Episode:2 Score: 50.0\n",
      "Episode:3 Score: 45.0\n",
      "Episode:4 Score: 55.0\n",
      "Episode:5 Score: 105.0\n",
      "Episode:6 Score: 165.0\n",
      "Episode:7 Score: 150.0\n",
      "Episode:8 Score: 85.0\n",
      "Episode:9 Score: 265.0\n",
      "Episode:10 Score: 155.0\n"
     ]
    }
   ],
   "source": [
    "episodes = 10\n",
    "for episode in range(1,episodes+1):\n",
    "  state=env.reset()\n",
    "  done=False\n",
    "  score=0\n",
    "\n",
    "  while not done:\n",
    "    env.render() # Rendering of the screen\n",
    "#     time.sleep(0.03) # Slowing the rendering. It stops 0.03 seconds for each frame\n",
    "    action = random.choice([0,1,2,3,4,5]) # There are 6 possible actions\n",
    "    n_state, reward, done, info = env.step(action)\n",
    "    score+=reward\n",
    "    sum_of_episodes +=score\n",
    "  print('Episode:{} Score: {}'.format(episode, score))\n",
    "print('Sum of all the random episodes:{}'.format(sum_of_episodes))\n",
    "env.close()   # Closing the video, otherwise the window will not close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "676b5265",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method step in module gym.wrappers.time_limit:\n",
      "\n",
      "step(action) method of gym.wrappers.time_limit.TimeLimit instance\n",
      "    Run one timestep of the environment's dynamics. When end of\n",
      "    episode is reached, you are responsible for calling `reset()`\n",
      "    to reset this environment's state.\n",
      "    \n",
      "    Accepts an action and returns a tuple (observation, reward, done, info).\n",
      "    \n",
      "    Args:\n",
      "        action (object): an action provided by the agent\n",
      "    \n",
      "    Returns:\n",
      "        observation (object): agent's observation of the current environment\n",
      "        reward (float) : amount of reward returned after previous action\n",
      "        done (bool): whether the episode has ended, in which case further step() calls will return undefined results\n",
      "        info (dict): contains auxiliary diagnostic information (helpful for debugging, and sometimes learning)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# help(env.step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac2346c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f1473a",
   "metadata": {},
   "source": [
    "# Create a Deep Learning Model with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827dd925",
   "metadata": {},
   "source": [
    "#### Ideally we want to take the score of each episode all the way up to 200. The deep RL is going to learn the best action to take in that specific environment to maximize the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f4ae8a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-07 19:16:41.393364: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/acr/anaconda3/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2021-12-07 19:16:41.393400: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf  #allows a sequential model with Keras\n",
    "# It's better to import tensorflow and use sequential than specifically importing the sequential from tensorlfow.keras.model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Convolution2D # We've imported the dense and flattten nodes. For the game we will give images of the game as input so a convolutional network will deal with the images\n",
    "from tensorflow.keras.optimizers import Adam # We've imported the optimizer Adam to train the deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ffcc151",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(height, width, channels, actions):\n",
    "    model =tf.keras.Sequential()  # initiating a sequential model\n",
    "    model.add(Convolution2D(32, (8,8), strides=(4,4), activation='relu', input_shape=(3, height, width, channels))) # As we will deal with images we start with a convolution network\n",
    "    # It will have 32 convolution2D filters. These are the number of filters that detect the various elements of the game\n",
    "    # The (8,8) is the size of the filters. In this case is 8 units by 8 units\n",
    "    # The strides is how much the filter moves for each step, in this case it will be a considreable distance as it moves 4 to the side and 4 up or down. it is a diagonal move\n",
    "    model.add(Convolution2D(64, (4,4), strides=(2,2), activation='relu')) # It passes through a flatten node which has the 4 different states\n",
    "    model.add(Convolution2D(64, (3,3), activation='relu')) # It passes through a flatten node which has the 4 different states\n",
    "    model.add(Flatten()) # Flatten layer. It takes all of our layers and flattens it into a single layer\n",
    "    model.add(Dense(512, activation='relu')) # Dense layers are fully connected layers. Every single unit is connected to every unit of the next layer\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear')) # This makes sure that the output are the actions. First (or through the top), come the images and lastly comes the actions\n",
    "    return model\n",
    "# This model is fed the states and gets out the actions, and it trains in order to maximize the reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "207002f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_21681/2115793928.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mmodel_a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model_a' is not defined"
     ]
    }
   ],
   "source": [
    "del model_a # To be used only when it gets to the dqn= build_agent. For some reason it gives an error when we don't delete the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "55d7e942",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_a = build_model(height, width, channels, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "19d1e86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 3, 51, 39, 32)     6176      \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 3, 24, 18, 64)     32832     \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 3, 22, 16, 64)     36928     \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 67584)             0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 512)               34603520  \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 6)                 1542      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 34,812,326\n",
      "Trainable params: 34,812,326\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_a.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d138f0",
   "metadata": {},
   "source": [
    "# Build Agent with Keras-RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "20107b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents import DQNAgent # Agent from keras rl, there are several\n",
    "from rl.memory import SequentialMemory # For the DQNAgent there is the need for some memory, the Sequential memory allow it\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy # EpsGreedyQPolicy is what finds the best reward outcome. The linear annealed policy gives a decay. As we get clsoes to the optimal value, we start to close in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6f4721d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "  policy_a = LinearAnnealedPolicy(EpsGreedyQPolicy(),attr='eps', value_max=1.,value_min=.1,value_test=.2, nb_steps=10000)\n",
    "  memory_a = SequentialMemory(limit=1000, window_length=3)\n",
    "  dqn = DQNAgent(model=model_a, memory=memory_a, policy=policy_a, \n",
    "                 enable_dueling_network=True, dueling_type='avg',\n",
    "                 nb_actions=actions, nb_steps_warmup=1000)\n",
    "  return dqn\n",
    "# Dueling networks split value and advantage, they help the model learn when to take action and when not to bother"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6ea65778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(DQNAgent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "05ad6fee",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 30000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/acr/anaconda3/lib/python3.9/site-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   650/30000: episode: 1, duration: 24.867s, episode steps: 650, steps per second:  26, episode reward: 155.000, mean reward:  0.238 [ 0.000, 30.000], mean action: 2.652 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n",
      "  1330/30000: episode: 2, duration: 25.579s, episode steps: 680, steps per second:  27, episode reward: 210.000, mean reward:  0.309 [ 0.000, 30.000], mean action: 2.550 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n",
      "  2145/30000: episode: 3, duration: 30.753s, episode steps: 815, steps per second:  27, episode reward: 225.000, mean reward:  0.276 [ 0.000, 30.000], mean action: 2.636 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n",
      "  2782/30000: episode: 4, duration: 25.212s, episode steps: 637, steps per second:  25, episode reward: 135.000, mean reward:  0.212 [ 0.000, 30.000], mean action: 2.634 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n",
      "  3787/30000: episode: 5, duration: 37.614s, episode steps: 1005, steps per second:  27, episode reward: 460.000, mean reward:  0.458 [ 0.000, 200.000], mean action: 2.522 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n",
      "  4440/30000: episode: 6, duration: 24.408s, episode steps: 653, steps per second:  27, episode reward: 190.000, mean reward:  0.291 [ 0.000, 30.000], mean action: 2.773 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/acr/anaconda3/lib/python3.9/site-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5197/30000: episode: 7, duration: 385.565s, episode steps: 757, steps per second:   2, episode reward: 125.000, mean reward:  0.165 [ 0.000, 30.000], mean action: 2.762 [0.000, 5.000],  loss: 1646.701584, mean_q: 18.371023, mean_eps: 0.541135\n",
      "  6643/30000: episode: 8, duration: 2597.059s, episode steps: 1446, steps per second:   1, episode reward: 720.000, mean reward:  0.498 [ 0.000, 200.000], mean action: 2.405 [0.000, 5.000],  loss: 5.378485, mean_q: 10.535580, mean_eps: 0.467245\n",
      "  7231/30000: episode: 9, duration: 1032.740s, episode steps: 588, steps per second:   1, episode reward: 55.000, mean reward:  0.094 [ 0.000, 20.000], mean action: 2.347 [0.000, 5.000],  loss: 1.879108, mean_q: 9.311108, mean_eps: 0.375715\n",
      "  7622/30000: episode: 10, duration: 687.665s, episode steps: 391, steps per second:   1, episode reward: 60.000, mean reward:  0.153 [ 0.000, 25.000], mean action: 2.691 [0.000, 5.000],  loss: 0.713542, mean_q: 10.241191, mean_eps: 0.331660\n",
      "  8554/30000: episode: 11, duration: 1636.598s, episode steps: 932, steps per second:   1, episode reward: 195.000, mean reward:  0.209 [ 0.000, 30.000], mean action: 2.564 [0.000, 5.000],  loss: 0.535043, mean_q: 11.189361, mean_eps: 0.272125\n",
      "  9145/30000: episode: 12, duration: 1032.608s, episode steps: 591, steps per second:   1, episode reward: 80.000, mean reward:  0.135 [ 0.000, 20.000], mean action: 2.272 [0.000, 5.000],  loss: 0.353196, mean_q: 9.106658, mean_eps: 0.203590\n",
      "  9831/30000: episode: 13, duration: 1197.241s, episode steps: 686, steps per second:   1, episode reward: 125.000, mean reward:  0.182 [ 0.000, 20.000], mean action: 2.434 [0.000, 5.000],  loss: 0.325853, mean_q: 9.744920, mean_eps: 0.146125\n",
      " 10352/30000: episode: 14, duration: 933.423s, episode steps: 521, steps per second:   1, episode reward: 40.000, mean reward:  0.077 [ 0.000, 15.000], mean action: 2.466 [0.000, 5.000],  loss: 0.499314, mean_q: 10.645761, mean_eps: 0.102481\n",
      "done, took 11521.127 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1b904354c0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn = build_agent(model_a,actions) # Agent initiated. It knows the model, where it has the environment, and the actions it is allowed to do \n",
    "dqn.compile(Adam(learning_rate=1e-4)) # Compile the model through optimizer\n",
    "dqn.fit(env, nb_steps=30000, visualize=False, verbose=2)\n",
    "# As the deep learning model is huge it takes a long time to be trained and it is recommended 10 million steps to train it well. It can take a long long time, so we just train a few steps to see improvements but not the ultimate model\n",
    "# The mean reward is what we want to maximize. It will bounce around for a while but will begin to converge the longer it is trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e1222f35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 90.000, steps: 650\n",
      "Episode 2: reward: 175.000, steps: 1346\n",
      "Episode 3: reward: 80.000, steps: 637\n",
      "Episode 4: reward: 280.000, steps: 1139\n",
      "Episode 5: reward: 150.000, steps: 977\n",
      "Episode 6: reward: 165.000, steps: 982\n",
      "Episode 7: reward: 650.000, steps: 1700\n",
      "Episode 8: reward: 290.000, steps: 985\n",
      "Episode 9: reward: 165.000, steps: 865\n",
      "Episode 10: reward: 440.000, steps: 1703\n",
      "Average reward:248.5\n",
      "Sum of the rewards:2485.0\n"
     ]
    }
   ],
   "source": [
    "scores = dqn.test(env,nb_episodes=10, visualize=False)\n",
    "print('Average reward:{}'.format(np.mean(scores.history['episode_reward'])))\n",
    "print('Sum of the rewards:{}'.format(np.sum(scores.history['episode_reward'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fbf7f4b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'episode_reward': [120.0,\n",
       "  25.0,\n",
       "  280.0,\n",
       "  230.0,\n",
       "  220.0,\n",
       "  385.0,\n",
       "  470.0,\n",
       "  310.0,\n",
       "  190.0,\n",
       "  155.0],\n",
       " 'nb_steps': [474, 329, 933, 902, 615, 668, 1245, 712, 681, 826]}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7f3e55",
   "metadata": {},
   "source": [
    "# Reloading Agent from Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "59470b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights('SavedWeights/10k-3hours/dqn_weights.h5f',overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f3e5f321",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_a\n",
    "del dqn\n",
    "model_a = build_model(height, width, channels, actions)\n",
    "dqn = build_agent(model_a,actions)\n",
    "dqn.compile(Adam(learning_rate=1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "73a62571",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dqn.load_weights('SavedWeights/1m/dqn_weights.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9dbf4180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 120.000, steps: 474\n",
      "Episode 2: reward: 25.000, steps: 329\n",
      "Episode 3: reward: 280.000, steps: 933\n",
      "Episode 4: reward: 230.000, steps: 902\n",
      "Episode 5: reward: 220.000, steps: 615\n",
      "Episode 6: reward: 385.000, steps: 668\n",
      "Episode 7: reward: 470.000, steps: 1245\n",
      "Episode 8: reward: 310.000, steps: 712\n",
      "Episode 9: reward: 190.000, steps: 681\n",
      "Episode 10: reward: 155.000, steps: 826\n",
      "Average reward:238.5\n",
      "Sum of the rewards:2385.0\n"
     ]
    }
   ],
   "source": [
    "scores = dqn.test(env,nb_episodes=10, visualize=False)\n",
    "print('Average reward:{}'.format(np.mean(scores.history['episode_reward'])))\n",
    "print('Sum of the rewards:{}'.format(np.sum(scores.history['episode_reward'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fde615b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e4bcc5ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/acr/Documents/Projeto Sistemas Mecatrónicos/RL-SistMecatron'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

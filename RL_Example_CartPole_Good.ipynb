{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9e2a147b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/acr/Documents/Jupyter Notebook'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ec4ef092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/acr/Documents/Jupyter Notebook\n"
     ]
    }
   ],
   "source": [
    "%cd /home/acr/Documents/Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e052221",
   "metadata": {},
   "source": [
    "# Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d0086cbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==2.3.0 (from versions: 2.5.0rc0, 2.5.0rc1, 2.5.0rc2, 2.5.0rc3, 2.5.0, 2.5.1, 2.5.2, 2.6.0rc0, 2.6.0rc1, 2.6.0rc2, 2.6.0, 2.6.1, 2.6.2, 2.7.0rc0, 2.7.0rc1, 2.7.0)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for tensorflow==2.3.0\u001b[0m\n",
      "Requirement already satisfied: gym in /home/acr/anaconda3/lib/python3.9/site-packages (0.21.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /home/acr/anaconda3/lib/python3.9/site-packages (from gym) (1.20.3)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/acr/anaconda3/lib/python3.9/site-packages (from gym) (2.0.0)\n",
      "Requirement already satisfied: keras in /home/acr/anaconda3/lib/python3.9/site-packages (2.7.0)\n",
      "Requirement already satisfied: keras-rl2 in /home/acr/anaconda3/lib/python3.9/site-packages (1.0.5)\n",
      "Requirement already satisfied: tensorflow in /home/acr/anaconda3/lib/python3.9/site-packages (from keras-rl2) (2.7.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.32.0 in /home/acr/anaconda3/lib/python3.9/site-packages (from tensorflow->keras-rl2) (0.37.0)\n",
      "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /home/acr/anaconda3/lib/python3.9/site-packages (from tensorflow->keras-rl2) (2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/acr/anaconda3/lib/python3.9/site-packages (from tensorflow->keras-rl2) (1.42.0)\n",
      "Requirement already satisfied: libclang>=9.0.1 in /home/acr/anaconda3/lib/python3.9/site-packages (from tensorflow->keras-rl2) (12.0.0)\n",
      "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /home/acr/anaconda3/lib/python3.9/site-packages (from tensorflow->keras-rl2) (0.4.0)\n",
      "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /home/acr/anaconda3/lib/python3.9/site-packages (from tensorflow->keras-rl2) (2.7.0)\n",
      "Requirement already satisfied: tensorboard~=2.6 in /home/acr/anaconda3/lib/python3.9/site-packages (from tensorflow->keras-rl2) (2.7.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/acr/anaconda3/lib/python3.9/site-packages (from tensorflow->keras-rl2) (1.12.1)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /home/acr/anaconda3/lib/python3.9/site-packages (from tensorflow->keras-rl2) (1.20.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/acr/anaconda3/lib/python3.9/site-packages (from tensorflow->keras-rl2) (3.10.0.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/acr/anaconda3/lib/python3.9/site-packages (from tensorflow->keras-rl2) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/acr/anaconda3/lib/python3.9/site-packages (from tensorflow->keras-rl2) (1.1.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /home/acr/anaconda3/lib/python3.9/site-packages (from tensorflow->keras-rl2) (3.19.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /home/acr/anaconda3/lib/python3.9/site-packages (from tensorflow->keras-rl2) (0.22.0)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in /home/acr/anaconda3/lib/python3.9/site-packages (from tensorflow->keras-rl2) (1.0.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /home/acr/anaconda3/lib/python3.9/site-packages (from tensorflow->keras-rl2) (1.1.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/acr/anaconda3/lib/python3.9/site-packages (from tensorflow->keras-rl2) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/acr/anaconda3/lib/python3.9/site-packages (from tensorflow->keras-rl2) (3.3.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /home/acr/anaconda3/lib/python3.9/site-packages (from tensorflow->keras-rl2) (2.7.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/acr/anaconda3/lib/python3.9/site-packages (from tensorflow->keras-rl2) (1.6.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/acr/anaconda3/lib/python3.9/site-packages (from tensorflow->keras-rl2) (3.3.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/acr/anaconda3/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow->keras-rl2) (0.4.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/acr/anaconda3/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow->keras-rl2) (2.26.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/acr/anaconda3/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow->keras-rl2) (2.3.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/acr/anaconda3/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow->keras-rl2) (58.0.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/acr/anaconda3/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow->keras-rl2) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/acr/anaconda3/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow->keras-rl2) (1.8.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/acr/anaconda3/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow->keras-rl2) (2.0.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/acr/anaconda3/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow->keras-rl2) (3.3.6)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/acr/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow->keras-rl2) (4.2.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/acr/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow->keras-rl2) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/acr/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow->keras-rl2) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/acr/anaconda3/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow->keras-rl2) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/acr/anaconda3/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow->keras-rl2) (4.8.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/acr/anaconda3/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow->keras-rl2) (3.6.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/acr/anaconda3/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow->keras-rl2) (0.4.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/acr/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow->keras-rl2) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/acr/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow->keras-rl2) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/acr/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow->keras-rl2) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/acr/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow->keras-rl2) (3.2)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/acr/anaconda3/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow->keras-rl2) (3.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.3.0\n",
    "!pip install gym\n",
    "!pip install keras\n",
    "!pip install keras-rl2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66992cdd",
   "metadata": {},
   "source": [
    "# Creating a random environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cada7c7",
   "metadata": {},
   "source": [
    "#### The goal is to balance the pole up, moving the cart to the left or right . For each step it takes a point, for a maximum of 200 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "647f29d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c211a5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "states = env.observation_space.shape[0]\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c390347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states, actions # There are 4 possible states and 2 possible actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66b5b20",
   "metadata": {},
   "source": [
    "#### Now we're just setting some random episodes where there is no learning from each of the episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a0fa8a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score: 12.0 Info: {}\n",
      "Episode:2 Score: 24.0 Info: {}\n",
      "Episode:3 Score: 16.0 Info: {}\n",
      "Episode:4 Score: 14.0 Info: {}\n",
      "Episode:5 Score: 17.0 Info: {}\n",
      "Episode:6 Score: 10.0 Info: {}\n",
      "Episode:7 Score: 21.0 Info: {}\n",
      "Episode:8 Score: 36.0 Info: {}\n",
      "Episode:9 Score: 40.0 Info: {}\n",
      "Episode:10 Score: 43.0 Info: {}\n"
     ]
    }
   ],
   "source": [
    "episodes = 10\n",
    "for episode in range(1,episodes+1):\n",
    "  state=env.reset()\n",
    "  done=False\n",
    "  score=0\n",
    "\n",
    "  while not done:\n",
    "    env.render() # Rendering of the screen\n",
    "    time.sleep(0.03) # Slowing the rendering. It stops 0.03 seconds for each frame\n",
    "    action = random.choice([0,1]) # 0 and 1 represent movements like right and left\n",
    "    n_state, reward, done, info = env.step(action)\n",
    "    score+=reward\n",
    "    info = info\n",
    "  print('Episode:{} Score: {} Info: {}'.format(episode, score, info))\n",
    "env.close()   # Closing the video, otherwise the window will not close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4751f117",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method step in module gym.wrappers.time_limit:\n",
      "\n",
      "step(action) method of gym.wrappers.time_limit.TimeLimit instance\n",
      "    Run one timestep of the environment's dynamics. When end of\n",
      "    episode is reached, you are responsible for calling `reset()`\n",
      "    to reset this environment's state.\n",
      "    \n",
      "    Accepts an action and returns a tuple (observation, reward, done, info).\n",
      "    \n",
      "    Args:\n",
      "        action (object): an action provided by the agent\n",
      "    \n",
      "    Returns:\n",
      "        observation (object): agent's observation of the current environment\n",
      "        reward (float) : amount of reward returned after previous action\n",
      "        done (bool): whether the episode has ended, in which case further step() calls will return undefined results\n",
      "        info (dict): contains auxiliary diagnostic information (helpful for debugging, and sometimes learning)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(env.step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f1473a",
   "metadata": {},
   "source": [
    "# Create a Deep Learning Model with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827dd925",
   "metadata": {},
   "source": [
    "#### Ideally we want to take the score of each episode all the way up to 200. The deep RL is going to learn the best action to take in that specific environment to maximize the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3f4ae8a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow  #allows a sequential model with Keras\n",
    "# It's better to import tensorflow and use sequential than specifically importing the sequential from tensorlfow.keras.model\n",
    "# from tensorflow.keras.layers import Dense, Flatten # We've imported the dense and flattten nodes\n",
    "# from tensorflow.keras.optimizers import Adam # We've imported the optimizer Adam to train the deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7ffcc151",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(states, actions):\n",
    "  model = tensorflow.keras.Sequential()  # initiating a sequential model\n",
    "  model.add(Flatten(input_shape=(1,states))) # It passes through a flatten node which has the 4 different states\n",
    "  model.add(Dense(24, activation='relu'))\n",
    "  model.add(Dense(24, activation='relu'))\n",
    "  model.add(Dense(actions, activation='linear')) # This makes sure that the output are the actions. First (or through the top), come the states and lastly comes the acitons\n",
    "  return model\n",
    "# This model is fed the states and gets out the actions, and it trains in order to maximize the reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "55d7e942",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a = build_model(states,actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "19d1e86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_1 (Flatten)         (None, 4)                 0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 24)                120       \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 24)                600       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 2)                 50        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 770\n",
      "Trainable params: 770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_a.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d138f0",
   "metadata": {},
   "source": [
    "# Build Agent with Keras-RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "20107b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents import DQNAgent # Agent from keras rl, there are several\n",
    "from rl.policy import BoltzmannQPolicy # There are two types of policies, the value-based rl and policy-based rl. This case will use policy based policy. This is the boltzmann Qpolicy\n",
    "from rl.memory import SequentialMemory # For the DQNAgent there is the need for some memory, the Sequential memory allow it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6f4721d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "  policy_a = BoltzmannQPolicy()\n",
    "  memory_a = SequentialMemory(limit=50000, window_length=1)\n",
    "  dqn = DQNAgent(model=model_a, memory=memory_a, policy=policy_a, \n",
    "                 nb_actions=actions, nb_steps_warmup=1000, target_model_update=1e-2)\n",
    "  return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6ea65778",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class DQNAgent in module rl.agents.dqn:\n",
      "\n",
      "class DQNAgent(AbstractDQNAgent)\n",
      " |  DQNAgent(model, policy=None, test_policy=None, enable_double_dqn=False, enable_dueling_network=False, dueling_type='avg', *args, **kwargs)\n",
      " |  \n",
      " |  # Arguments\n",
      " |      model__: A Keras model.\n",
      " |      policy__: A Keras-rl policy that are defined in [policy](https://github.com/keras-rl/keras-rl/blob/master/rl/policy.py).\n",
      " |      test_policy__: A Keras-rl policy.\n",
      " |      enable_double_dqn__: A boolean which enable target network as a second network proposed by van Hasselt et al. to decrease overfitting.\n",
      " |      enable_dueling_dqn__: A boolean which enable dueling architecture proposed by Mnih et al.\n",
      " |      dueling_type__: If `enable_dueling_dqn` is set to `True`, a type of dueling architecture must be chosen which calculate Q(s,a) from V(s) and A(s,a) differently. Note that `avg` is recommanded in the [paper](https://arxiv.org/abs/1511.06581).\n",
      " |          `avg`: Q(s,a;theta) = V(s;theta) + (A(s,a;theta)-Avg_a(A(s,a;theta)))\n",
      " |          `max`: Q(s,a;theta) = V(s;theta) + (A(s,a;theta)-max_a(A(s,a;theta)))\n",
      " |          `naive`: Q(s,a;theta) = V(s;theta) + A(s,a;theta)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      DQNAgent\n",
      " |      AbstractDQNAgent\n",
      " |      rl.core.Agent\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, model, policy=None, test_policy=None, enable_double_dqn=False, enable_dueling_network=False, dueling_type='avg', *args, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  backward(self, reward, terminal)\n",
      " |      Updates the agent after having executed the action returned by `forward`.\n",
      " |      If the policy is implemented by a neural network, this corresponds to a weight update using back-prop.\n",
      " |      \n",
      " |      # Argument\n",
      " |          reward (float): The observed reward after executing the action returned by `forward`.\n",
      " |          terminal (boolean): `True` if the new state of the environment is terminal.\n",
      " |      \n",
      " |      # Returns\n",
      " |          List of metrics values\n",
      " |  \n",
      " |  compile(self, optimizer, metrics=[])\n",
      " |      Compiles an agent and the underlaying models to be used for training and testing.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          optimizer (`keras.optimizers.Optimizer` instance): The optimizer to be used during training.\n",
      " |          metrics (list of functions `lambda y_true, y_pred: metric`): The metrics to run during training.\n",
      " |  \n",
      " |  forward(self, observation)\n",
      " |      Takes the an observation from the environment and returns the action to be taken next.\n",
      " |      If the policy is implemented by a neural network, this corresponds to a forward (inference) pass.\n",
      " |      \n",
      " |      # Argument\n",
      " |          observation (object): The current observation from the environment.\n",
      " |      \n",
      " |      # Returns\n",
      " |          The next action to be executed in the environment.\n",
      " |  \n",
      " |  get_config(self)\n",
      " |      Configuration of the agent for serialization.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Dictionnary with agent configuration\n",
      " |  \n",
      " |  load_weights(self, filepath)\n",
      " |      Loads the weights of an agent from an HDF5 file.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          filepath (str): The path to the HDF5 file.\n",
      " |  \n",
      " |  reset_states(self)\n",
      " |      Resets all internally kept states after an episode is completed.\n",
      " |  \n",
      " |  save_weights(self, filepath, overwrite=False)\n",
      " |      Saves the weights of an agent as an HDF5 file.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          filepath (str): The path to where the weights should be saved.\n",
      " |          overwrite (boolean): If `False` and `filepath` already exists, raises an error.\n",
      " |  \n",
      " |  update_target_model_hard(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  layers\n",
      " |      Returns all layers of the underlying model(s).\n",
      " |      \n",
      " |      If the concrete implementation uses multiple internal models,\n",
      " |      this method returns them in a concatenated list.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A list of the model's layers\n",
      " |  \n",
      " |  metrics_names\n",
      " |      The human-readable names of the agent's metrics. Must return as many names as there\n",
      " |      are metrics (see also `compile`).\n",
      " |      \n",
      " |      # Returns\n",
      " |          A list of metric's names (string)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  policy\n",
      " |  \n",
      " |  test_policy\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from AbstractDQNAgent:\n",
      " |  \n",
      " |  compute_batch_q_values(self, state_batch)\n",
      " |  \n",
      " |  compute_q_values(self, state)\n",
      " |  \n",
      " |  process_state_batch(self, batch)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from rl.core.Agent:\n",
      " |  \n",
      " |  fit(self, env, nb_steps, action_repetition=1, callbacks=None, verbose=1, visualize=False, nb_max_start_steps=0, start_step_policy=None, log_interval=10000, nb_max_episode_steps=None)\n",
      " |      Trains the agent on the given environment.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          env: (`Env` instance): Environment that the agent interacts with. See [Env](#env) for details.\n",
      " |          nb_steps (integer): Number of training steps to be performed.\n",
      " |          action_repetition (integer): Number of times the agent repeats the same action without\n",
      " |              observing the environment again. Setting this to a value > 1 can be useful\n",
      " |              if a single action only has a very small effect on the environment.\n",
      " |          callbacks (list of `keras.callbacks.Callback` or `rl.callbacks.Callback` instances):\n",
      " |              List of callbacks to apply during training. See [callbacks](/callbacks) for details.\n",
      " |          verbose (integer): 0 for no logging, 1 for interval logging (compare `log_interval`), 2 for episode logging\n",
      " |          visualize (boolean): If `True`, the environment is visualized during training. However,\n",
      " |              this is likely going to slow down training significantly and is thus intended to be\n",
      " |              a debugging instrument.\n",
      " |          nb_max_start_steps (integer): Number of maximum steps that the agent performs at the beginning\n",
      " |              of each episode using `start_step_policy`. Notice that this is an upper limit since\n",
      " |              the exact number of steps to be performed is sampled uniformly from [0, max_start_steps]\n",
      " |              at the beginning of each episode.\n",
      " |          start_step_policy (`lambda observation: action`): The policy\n",
      " |              to follow if `nb_max_start_steps` > 0. If set to `None`, a random action is performed.\n",
      " |          log_interval (integer): If `verbose` = 1, the number of steps that are considered to be an interval.\n",
      " |          nb_max_episode_steps (integer): Number of steps per episode that the agent performs before\n",
      " |              automatically resetting the environment. Set to `None` if each episode should run\n",
      " |              (potentially indefinitely) until the environment signals a terminal state.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A `keras.callbacks.History` instance that recorded the entire training process.\n",
      " |  \n",
      " |  test(self, env, nb_episodes=1, action_repetition=1, callbacks=None, visualize=True, nb_max_episode_steps=None, nb_max_start_steps=0, start_step_policy=None, verbose=1)\n",
      " |      Callback that is called before training begins.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          env: (`Env` instance): Environment that the agent interacts with. See [Env](#env) for details.\n",
      " |          nb_episodes (integer): Number of episodes to perform.\n",
      " |          action_repetition (integer): Number of times the agent repeats the same action without\n",
      " |              observing the environment again. Setting this to a value > 1 can be useful\n",
      " |              if a single action only has a very small effect on the environment.\n",
      " |          callbacks (list of `keras.callbacks.Callback` or `rl.callbacks.Callback` instances):\n",
      " |              List of callbacks to apply during training. See [callbacks](/callbacks) for details.\n",
      " |          verbose (integer): 0 for no logging, 1 for interval logging (compare `log_interval`), 2 for episode logging\n",
      " |          visualize (boolean): If `True`, the environment is visualized during training. However,\n",
      " |              this is likely going to slow down training significantly and is thus intended to be\n",
      " |              a debugging instrument.\n",
      " |          nb_max_start_steps (integer): Number of maximum steps that the agent performs at the beginning\n",
      " |              of each episode using `start_step_policy`. Notice that this is an upper limit since\n",
      " |              the exact number of steps to be performed is sampled uniformly from [0, max_start_steps]\n",
      " |              at the beginning of each episode.\n",
      " |          start_step_policy (`lambda observation: action`): The policy\n",
      " |              to follow if `nb_max_start_steps` > 0. If set to `None`, a random action is performed.\n",
      " |          log_interval (integer): If `verbose` = 1, the number of steps that are considered to be an interval.\n",
      " |          nb_max_episode_steps (integer): Number of steps per episode that the agent performs before\n",
      " |              automatically resetting the environment. Set to `None` if each episode should run\n",
      " |              (potentially indefinitely) until the environment signals a terminal state.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A `keras.callbacks.History` instance that recorded the entire training process.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from rl.core.Agent:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(DQNAgent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "05ad6fee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 30000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 78s 8ms/step - reward: 1.0000\n",
      "53 episodes - episode_reward: 188.642 [151.000, 200.000] - loss: 2.555 - mae: 39.302 - mean_q: 79.218\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 88s 9ms/step - reward: 1.0000\n",
      "52 episodes - episode_reward: 190.135 [34.000, 200.000] - loss: 6.931 - mae: 41.677 - mean_q: 83.709\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 88s 9ms/step - reward: 1.0000\n",
      "done, took 254.463 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7faeb850f2e0>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn = build_agent(model_a,actions) # Agent initiated. It knows the model, where it has the environment, and the actions it is allowed to do \n",
    "dqn.compile(Adam(learning_rate=1e-3), metrics=['mae']) # Compile the model through optimizer\n",
    "dqn.fit(env, nb_steps=30000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e1222f35",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 100 episodes ...\n",
      "Episode 1: reward: 200.000, steps: 200\n",
      "Episode 2: reward: 200.000, steps: 200\n",
      "Episode 3: reward: 200.000, steps: 200\n",
      "Episode 4: reward: 200.000, steps: 200\n",
      "Episode 5: reward: 200.000, steps: 200\n",
      "Episode 6: reward: 200.000, steps: 200\n",
      "Episode 7: reward: 200.000, steps: 200\n",
      "Episode 8: reward: 200.000, steps: 200\n",
      "Episode 9: reward: 200.000, steps: 200\n",
      "Episode 10: reward: 200.000, steps: 200\n",
      "Episode 11: reward: 200.000, steps: 200\n",
      "Episode 12: reward: 200.000, steps: 200\n",
      "Episode 13: reward: 200.000, steps: 200\n",
      "Episode 14: reward: 200.000, steps: 200\n",
      "Episode 15: reward: 200.000, steps: 200\n",
      "Episode 16: reward: 200.000, steps: 200\n",
      "Episode 17: reward: 200.000, steps: 200\n",
      "Episode 18: reward: 200.000, steps: 200\n",
      "Episode 19: reward: 200.000, steps: 200\n",
      "Episode 20: reward: 200.000, steps: 200\n",
      "Episode 21: reward: 200.000, steps: 200\n",
      "Episode 22: reward: 200.000, steps: 200\n",
      "Episode 23: reward: 200.000, steps: 200\n",
      "Episode 24: reward: 200.000, steps: 200\n",
      "Episode 25: reward: 200.000, steps: 200\n",
      "Episode 26: reward: 200.000, steps: 200\n",
      "Episode 27: reward: 200.000, steps: 200\n",
      "Episode 28: reward: 200.000, steps: 200\n",
      "Episode 29: reward: 200.000, steps: 200\n",
      "Episode 30: reward: 200.000, steps: 200\n",
      "Episode 31: reward: 200.000, steps: 200\n",
      "Episode 32: reward: 200.000, steps: 200\n",
      "Episode 33: reward: 200.000, steps: 200\n",
      "Episode 34: reward: 200.000, steps: 200\n",
      "Episode 35: reward: 200.000, steps: 200\n",
      "Episode 36: reward: 200.000, steps: 200\n",
      "Episode 37: reward: 200.000, steps: 200\n",
      "Episode 38: reward: 200.000, steps: 200\n",
      "Episode 39: reward: 200.000, steps: 200\n",
      "Episode 40: reward: 200.000, steps: 200\n",
      "Episode 41: reward: 200.000, steps: 200\n",
      "Episode 42: reward: 200.000, steps: 200\n",
      "Episode 43: reward: 200.000, steps: 200\n",
      "Episode 44: reward: 200.000, steps: 200\n",
      "Episode 45: reward: 200.000, steps: 200\n",
      "Episode 46: reward: 200.000, steps: 200\n",
      "Episode 47: reward: 200.000, steps: 200\n",
      "Episode 48: reward: 200.000, steps: 200\n",
      "Episode 49: reward: 200.000, steps: 200\n",
      "Episode 50: reward: 200.000, steps: 200\n",
      "Episode 51: reward: 200.000, steps: 200\n",
      "Episode 52: reward: 200.000, steps: 200\n",
      "Episode 53: reward: 200.000, steps: 200\n",
      "Episode 54: reward: 200.000, steps: 200\n",
      "Episode 55: reward: 200.000, steps: 200\n",
      "Episode 56: reward: 200.000, steps: 200\n",
      "Episode 57: reward: 200.000, steps: 200\n",
      "Episode 58: reward: 200.000, steps: 200\n",
      "Episode 59: reward: 200.000, steps: 200\n",
      "Episode 60: reward: 200.000, steps: 200\n",
      "Episode 61: reward: 200.000, steps: 200\n",
      "Episode 62: reward: 200.000, steps: 200\n",
      "Episode 63: reward: 200.000, steps: 200\n",
      "Episode 64: reward: 200.000, steps: 200\n",
      "Episode 65: reward: 200.000, steps: 200\n",
      "Episode 66: reward: 200.000, steps: 200\n",
      "Episode 67: reward: 200.000, steps: 200\n",
      "Episode 68: reward: 200.000, steps: 200\n",
      "Episode 69: reward: 200.000, steps: 200\n",
      "Episode 70: reward: 200.000, steps: 200\n",
      "Episode 71: reward: 200.000, steps: 200\n",
      "Episode 72: reward: 200.000, steps: 200\n",
      "Episode 73: reward: 200.000, steps: 200\n",
      "Episode 74: reward: 200.000, steps: 200\n",
      "Episode 75: reward: 200.000, steps: 200\n",
      "Episode 76: reward: 200.000, steps: 200\n",
      "Episode 77: reward: 200.000, steps: 200\n",
      "Episode 78: reward: 200.000, steps: 200\n",
      "Episode 79: reward: 200.000, steps: 200\n",
      "Episode 80: reward: 200.000, steps: 200\n",
      "Episode 81: reward: 200.000, steps: 200\n",
      "Episode 82: reward: 200.000, steps: 200\n",
      "Episode 83: reward: 200.000, steps: 200\n",
      "Episode 84: reward: 200.000, steps: 200\n",
      "Episode 85: reward: 200.000, steps: 200\n",
      "Episode 86: reward: 200.000, steps: 200\n",
      "Episode 87: reward: 200.000, steps: 200\n",
      "Episode 88: reward: 200.000, steps: 200\n",
      "Episode 89: reward: 200.000, steps: 200\n",
      "Episode 90: reward: 200.000, steps: 200\n",
      "Episode 91: reward: 200.000, steps: 200\n",
      "Episode 92: reward: 200.000, steps: 200\n",
      "Episode 93: reward: 200.000, steps: 200\n",
      "Episode 94: reward: 200.000, steps: 200\n",
      "Episode 95: reward: 200.000, steps: 200\n",
      "Episode 96: reward: 200.000, steps: 200\n",
      "Episode 97: reward: 200.000, steps: 200\n",
      "Episode 98: reward: 200.000, steps: 200\n",
      "Episode 99: reward: 200.000, steps: 200\n",
      "Episode 100: reward: 200.000, steps: 200\n",
      "200.0\n"
     ]
    }
   ],
   "source": [
    "scores = dqn.test(env,nb_episodes=100, visualize=False)\n",
    "print(np.mean(scores.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0c9c6dab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 20 episodes ...\n",
      "Episode 1: reward: 200.000, steps: 200\n",
      "Episode 2: reward: 200.000, steps: 200\n",
      "Episode 3: reward: 200.000, steps: 200\n",
      "Episode 4: reward: 200.000, steps: 200\n",
      "Episode 5: reward: 200.000, steps: 200\n",
      "Episode 6: reward: 200.000, steps: 200\n",
      "Episode 7: reward: 200.000, steps: 200\n",
      "Episode 8: reward: 200.000, steps: 200\n",
      "Episode 9: reward: 200.000, steps: 200\n",
      "Episode 10: reward: 200.000, steps: 200\n",
      "Episode 11: reward: 200.000, steps: 200\n",
      "Episode 12: reward: 200.000, steps: 200\n",
      "Episode 13: reward: 200.000, steps: 200\n",
      "Episode 14: reward: 200.000, steps: 200\n",
      "Episode 15: reward: 200.000, steps: 200\n",
      "Episode 16: reward: 200.000, steps: 200\n",
      "Episode 17: reward: 200.000, steps: 200\n",
      "Episode 18: reward: 200.000, steps: 200\n",
      "Episode 19: reward: 200.000, steps: 200\n",
      "Episode 20: reward: 200.000, steps: 200\n"
     ]
    }
   ],
   "source": [
    "import rl.callbacks\n",
    "class EpisodeLogger(rl.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        self.observations = {}\n",
    "        self.rewards = {}\n",
    "        self.actions = {}\n",
    "\n",
    "    def on_episode_begin(self, episode, logs):\n",
    "        self.observations[episode] = []\n",
    "        self.rewards[episode] = []\n",
    "        self.actions[episode] = []\n",
    "\n",
    "    def on_step_end(self, step, logs):\n",
    "        episode = logs['episode']\n",
    "        self.observations[episode].append(logs['observation'])\n",
    "        self.rewards[episode].append(logs['reward'])\n",
    "        self.actions[episode].append(logs['action'])\n",
    "\n",
    "cb_ep = EpisodeLogger()\n",
    "# _ = dqn.test(env,nb_episodes=15, callbacks=[cb_ep], visualize=True)\n",
    "\n",
    "dqn.test(env, nb_episodes=20, visualize=True)\n",
    "\n",
    "## Tenho de ver como diminuir a velocidade do rendering. O time.sleep nÃ£o resulta\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7a42b49c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method test in module rl.core:\n",
      "\n",
      "test(env, nb_episodes=1, action_repetition=1, callbacks=None, visualize=True, nb_max_episode_steps=None, nb_max_start_steps=0, start_step_policy=None, verbose=1) method of rl.agents.dqn.DQNAgent instance\n",
      "    Callback that is called before training begins.\n",
      "    \n",
      "    # Arguments\n",
      "        env: (`Env` instance): Environment that the agent interacts with. See [Env](#env) for details.\n",
      "        nb_episodes (integer): Number of episodes to perform.\n",
      "        action_repetition (integer): Number of times the agent repeats the same action without\n",
      "            observing the environment again. Setting this to a value > 1 can be useful\n",
      "            if a single action only has a very small effect on the environment.\n",
      "        callbacks (list of `keras.callbacks.Callback` or `rl.callbacks.Callback` instances):\n",
      "            List of callbacks to apply during training. See [callbacks](/callbacks) for details.\n",
      "        verbose (integer): 0 for no logging, 1 for interval logging (compare `log_interval`), 2 for episode logging\n",
      "        visualize (boolean): If `True`, the environment is visualized during training. However,\n",
      "            this is likely going to slow down training significantly and is thus intended to be\n",
      "            a debugging instrument.\n",
      "        nb_max_start_steps (integer): Number of maximum steps that the agent performs at the beginning\n",
      "            of each episode using `start_step_policy`. Notice that this is an upper limit since\n",
      "            the exact number of steps to be performed is sampled uniformly from [0, max_start_steps]\n",
      "            at the beginning of each episode.\n",
      "        start_step_policy (`lambda observation: action`): The policy\n",
      "            to follow if `nb_max_start_steps` > 0. If set to `None`, a random action is performed.\n",
      "        log_interval (integer): If `verbose` = 1, the number of steps that are considered to be an interval.\n",
      "        nb_max_episode_steps (integer): Number of steps per episode that the agent performs before\n",
      "            automatically resetting the environment. Set to `None` if each episode should run\n",
      "            (potentially indefinitely) until the environment signals a terminal state.\n",
      "    \n",
      "    # Returns\n",
      "        A `keras.callbacks.History` instance that recorded the entire training process.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(dqn.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "59470b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights('CartPole_DQN_Weights_Good',overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f3e5f321",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_a\n",
    "del dqn\n",
    "del env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "122151a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "env=gym.make('CartPole-v0')\n",
    "actions = env.action_space.n\n",
    "states = env.observation_space.shape[0]\n",
    "model_a = build_model(states,actions)\n",
    "dqn = build_agent(model_a,actions)\n",
    "dqn.compile(Adam(lr=1e-3),metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "73a62571",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.load_weights('CartPole_DQN_Weights_Good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9dbf4180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 15 episodes ...\n",
      "Episode 1: reward: 200.000, steps: 200\n",
      "Episode 2: reward: 200.000, steps: 200\n",
      "Episode 3: reward: 200.000, steps: 200\n",
      "Episode 4: reward: 200.000, steps: 200\n",
      "Episode 5: reward: 200.000, steps: 200\n",
      "Episode 6: reward: 200.000, steps: 200\n",
      "Episode 7: reward: 200.000, steps: 200\n",
      "Episode 8: reward: 200.000, steps: 200\n",
      "Episode 9: reward: 200.000, steps: 200\n",
      "Episode 10: reward: 200.000, steps: 200\n",
      "Episode 11: reward: 200.000, steps: 200\n",
      "Episode 12: reward: 200.000, steps: 200\n",
      "Episode 13: reward: 200.000, steps: 200\n",
      "Episode 14: reward: 200.000, steps: 200\n",
      "Episode 15: reward: 200.000, steps: 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fae997d2ac0>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes=15, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "fde615b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
